<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes"
    />
    <title>M 340L | Eigenstuffs</title>
    <link rel="stylesheet" href="/M340L/M340L.css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
      type="text/javascript"
    ></script>
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <h1>Note Contents</h1>
    <nav id="TOC" role="doc-toc">
      <ul>
        <li>
          <a href="#eigenstuffs" id="toc-eigenstuffs">Eigenstuffs</a>
          <ul>
            <li>
              <a
                href="#intuition-behind-eigenstuff"
                id="toc-intuition-behind-eigenstuff"
                >Intuition Behind Eigenstuff</a
              >
            </li>
            <li>
              <a
                href="#finding-eigenvalues-and-eigenvectors"
                id="toc-finding-eigenvalues-and-eigenvectors"
                >Finding Eigenvalues and Eigenvectors</a
              >
            </li>
            <li>
              <a href="#changing-a" id="toc-changing-a"
                >Changing <span class="math inline">\(A\)</span></a
              >
            </li>
            <li>
              <a href="#repeated-eigenvalues" id="toc-repeated-eigenvalues"
                >Repeated Eigenvalues</a
              >
            </li>
            <li>
              <a href="#diagonalization" id="toc-diagonalization"
                >Diagonalization</a
              >
            </li>
            <li>
              <a href="#diagonalizability" id="toc-diagonalizability"
                >Diagonalizability</a
              >
            </li>
            <li>
              <a href="#powers-of-a" id="toc-powers-of-a"
                >Powers of <span class="math inline">\(A\)</span></a
              >
            </li>
          </ul>
        </li>
      </ul>
    </nav>
    <h1 id="eigenstuffs">Eigenstuffs</h1>
    <h2 id="intuition-behind-eigenstuff">Intuition Behind Eigenstuff</h2>
    <p>
      Suppose there exists some matrix <span class="math inline">\(A\)</span>.
      Applying <span class="math inline">\(Ax\)</span> for some vector
      <span class="math inline">\(x\)</span> with applicable dimensions gives a
      result, <span class="math inline">\(b \in \mathcal{C}(A)\)</span>. Suppose
      <span class="math inline">\(b=\lambda x\)</span> for some constant
      <span class="math inline">\(\lambda\)</span>. Then, the result,
      <span class="math inline">\(b\)</span>, is a multiple of the input,
      <span class="math inline">\(x\)</span>, and we can see that the
      transformation encoded by <span class="math inline">\(A\)</span> has acted
      upon <span class="math inline">\(x\)</span> in a rather special way. Then,
      we call <span class="math inline">\(x\)</span> an eigenvector of
      <span class="math inline">\(A\)</span>. This scaling factor
      <span class="math inline">\(\lambda\)</span> is called an eigenvalue of
      <span class="math inline">\(A\)</span>.
    </p>
    <h2 id="finding-eigenvalues-and-eigenvectors">
      Finding Eigenvalues and Eigenvectors
    </h2>
    <p>The fundamental equation is:</p>
    <p><span class="math display">\[ Ax=\lambda x \]</span></p>
    <p>
      From which, we can find the eigenvalues for
      <span class="math inline">\(A\)</span> by deriving as follows:
    </p>
    <p>
      <span class="math display">\[Ax=\lambda x\]</span>
      <span class="math display">\[Ax-\lambda x=0\]</span>
      <span class="math display">\[(A-\lambda I)x=0 \tag{1}\]</span>
      <span class="math display">\[\det{(A-\lambda I)}=0 \tag{2}\]</span>
    </p>
    <p>
      This, <span class="math inline">\((2)\)</span>, is the characteristic
      polynomial (notated as
      <span class="math inline">\(\textrm{PA}(A)\)</span>), a way to solve for
      all eigenvalues of <span class="math inline">\(A\)</span>.
    </p>
    <p>
      Once all eigenvalues are found, we can plug each into
      <span class="math inline">\((1)\)</span> to find the corresponding
      eigenvectors.
    </p>
    <h2 id="changing-a">Changing <span class="math inline">\(A\)</span></h2>
    <p>
      Suppose <span class="math inline">\(A\)</span> has eigenvalue
      <span class="math inline">\(\lambda\)</span> and eigenvector
      <span class="math inline">\(x\)</span>. Then,
    </p>
    <p>
      <span class="math display">\[Ax=\lambda x\]</span>
      <span class="math display">\[(A+cI)x=\lambda x+cIx\]</span>
      <span class="math display">\[(A+cI)x=(\lambda +cI)x\]</span>
    </p>
    <p>
      So, adding a multiple of <span class="math inline">\(I\)</span> to
      <span class="math inline">\(A\)</span> changes only the eigenvalues by
      that multiple. It does nothing to the eigenvectors. Note, however, that
      this does not work for an arbitrary
      <span class="math inline">\(B\)</span> with eigenvalue
      <span class="math inline">\(\alpha\)</span> because we do not know that
      <span class="math inline">\(A\)</span> and
      <span class="math inline">\(B\)</span> share the eigenvector
      <span class="math inline">\(x\)</span>. Similarly, we cannot gain new
      information about <span class="math inline">\(AB\)</span>.
    </p>
    <h2 id="repeated-eigenvalues">Repeated Eigenvalues</h2>
    <p>Consider the matrix</p>
    <p>
      <span class="math display"
        >\[ A= \begin{bmatrix} 3 &amp; 1 \\ 0 &amp; 3 \end{bmatrix} \]</span
      >
    </p>
    <p>Then,</p>
    <p>
      <span class="math display"
        >\[ \begin{align*} \det{(A-\lambda I)} &amp;= \begin{vmatrix} 3-\lambda
        &amp; 1 \\ 0 &amp; 3-\lambda \end{vmatrix} \\ &amp;= (3-\lambda)^2 \\
        &amp;\therefore \lambda_1=\lambda_2=3 \end{align*} \]</span
      >
    </p>
    <p>
      However, when we find <span class="math inline">\(N(A-3I)\)</span>, we
      discover that it only has dimension
      <span class="math inline">\(1\)</span>. In this case, we say that the
      eigenspace for <span class="math inline">\(\lambda=3\)</span> is of
      dimension <span class="math inline">\(1\)</span>. Thus, there do not exist
      <span class="math inline">\(2\)</span> independent eigenvectors for
      <span class="math inline">\(A\)</span>. It is degenerate, and formally, is
      not diagonalizable.
    </p>
    <p>
      Though, there are cases of non-degenerate (diagonalizable)
      <span class="math inline">\(A\)</span> matrices with repeated eigenvalues.
      Consider <span class="math inline">\(I\)</span>. In these cases, there are
      still <span class="math inline">\(n\)</span> (for
      <span class="math inline">\(m \times n\)</span>) independent eigenvectors
      for <span class="math inline">\(A\)</span>.
    </p>
    <h2 id="diagonalization">Diagonalization</h2>
    <p>
      Consider a matrix <span class="math inline">\(A\)</span> which is
      <span class="math inline">\(n \times n\)</span> and has
      <span class="math inline">\(n\)</span> independent eigenvectors.
    </p>
    <p>Then, it is possible to construct:</p>
    <p><span class="math display">\[S^{-1}AS=\Lambda\]</span></p>
    <p>
      where <span class="math inline">\(S\)</span> is a matrix of eigenvectors
      and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix of
      corresponding eigenvalues.
    </p>
    <p>
      To show this, suppose there exist
      <span class="math inline">\(n\)</span> independent eigenvectors of a
      matrix <span class="math inline">\(A\)</span>. Put them in the columns of
      a matrix, <span class="math inline">\(S\)</span>.
    </p>
    <p>
      <span class="math display"
        >\[ \begin{align*} AS &amp;= A \begin{bmatrix} \vert &amp; \hspace{1em}
        &amp; \vert \\ x_1 &amp; \cdots &amp; x_n \\ \vert &amp; \hspace{1em}
        &amp; \vert \end{bmatrix} \\ &amp;= \begin{bmatrix} \vert &amp;
        \hspace{1em} &amp; \vert \\ \lambda_1 x_1 &amp; \cdots &amp; \lambda_n
        x_n \\ \vert &amp; \hspace{1em} &amp; \vert \end{bmatrix} \\ &amp;=
        \begin{bmatrix} \vert &amp; \hspace{1em} &amp; \vert \\ x_1 &amp; \cdots
        &amp; x_n \\ \vert &amp; \hspace{1em} &amp; \vert \end{bmatrix}
        \begin{bmatrix} \lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp;
        \lambda_2 &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots
        &amp; \vdots \\ 0 &amp; \cdots &amp; \cdots &amp; \lambda_n
        \end{bmatrix} \\ &amp;= S \Lambda \end{align*} \]</span
      >
    </p>
    <p>
      Then, simply take the left inverse of
      <span class="math inline">\(S\)</span> to get:
    </p>
    <p><span class="math display">\[S^{-1}AS=\Lambda\]</span></p>
    <p>However, we could just as easily construct:</p>
    <p><span class="math display">\[A=S \Lambda S^{-1}\]</span></p>
    <p>
      Which is an eigendecomposition of <span class="math inline">\(A\)</span>.
      There are different special names for this decomposition for different
      kinds of <span class="math inline">\(S\)</span> and
      <span class="math inline">\(A\)</span>.
    </p>
    <h2 id="diagonalizability">Diagonalizability</h2>
    <p>
      <span class="math inline">\(A\)</span> is diagonalizable when it has
      <span class="math inline">\(n\)</span> independent eigenvectors.
      <span class="math inline">\(A\)</span> is sure to have
      <span class="math inline">\(n\)</span> independent eigenvectors if all
      eigenvalues are unique. Thus, <span class="math inline">\(A\)</span> is
      diagonalizable if all eigenvalues are unique.
    </p>
    <p>
      However, it is also possible to have an
      <span class="math inline">\(A\)</span> without unique eigenvalues that
      contains <span class="math inline">\(n\)</span> unique eigenvectors. But,
      to get <span class="math inline">\(n\)</span> independent eigenvectors, we
      get that
    </p>
    <p>
      <span class="math display"
        >\[\text{multiplicity of } \lambda = \dim{N(A-\lambda I)}\]</span
      >
    </p>
    <h2 id="powers-of-a">Powers of <span class="math inline">\(A\)</span></h2>
    <p>Let <span class="math inline">\(Ax=\lambda x\)</span>. Then,</p>
    <p><span class="math display">\[A^2x=\lambda Ax=\lambda^2x\]</span></p>
    <p>This generalizes to:</p>
    <p><span class="math display">\[A^kx=\lambda^kx\]</span></p>
    <p>
      Thus, the power of <span class="math inline">\(A\)</span> only influences
      the eigenvalues of <span class="math inline">\(A\)</span>, not the
      eigenvectors.
    </p>
    <p>
      Similarly, we find that if
      <span class="math inline">\(A=S\Lambda S^{-1}\)</span>:
    </p>
    <p>
      <span class="math display"
        >\[ \begin{align*} A^2&amp;=S\Lambda \cancel{S^{-1}S}\Lambda S^{-1} \\
        &amp;= S \Lambda^2 S^{-1} \end{align*} \]</span
      >
    </p>
    <p>Which generalizes to:</p>
    <p><span class="math display">\[A^k=S \Lambda^k S^{-1}\]</span></p>
  </body>
</html>
